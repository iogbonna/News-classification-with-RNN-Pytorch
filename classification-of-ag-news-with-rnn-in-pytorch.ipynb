{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "\n",
    "with zipfile.ZipFile('data/data.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This notebook implements a text classification model to categorize AG News dataset articles into four predefined classes using a Recurrent Neural Network (RNN) built with PyTorch. The workflow includes preprocessing text data, tokenizing and encoding text sequences, and training an RNN-based classifier with embeddings, sequential modeling, and mean pooling to predict article categories. The notebook demonstrates key concepts in natural language processing (NLP) and deep learning, making it a valuable resource for learning text classification techniques with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T20:15:05.335214Z",
     "iopub.status.busy": "2024-12-08T20:15:05.334368Z",
     "iopub.status.idle": "2024-12-08T20:15:18.129861Z",
     "shell.execute_reply": "2024-12-08T20:15:18.127652Z",
     "shell.execute_reply.started": "2024-12-08T20:15:05.335169Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install torchtext==0.5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import all required packages\n",
    "\n",
    "Imports several essential libraries for text processing and deep learning tasks, particularly for natural language processing (NLP). PyTorch (torch) is used as the core library for building and training neural networks, with utilities like Dataset and DataLoader from torch.utils.data to handle datasets, batching, and data shuffling efficiently. The torchtext library and its get_tokenizer function facilitate text preprocessing, tokenization, and vocabulary management, supporting various tokenization methods like basic_english. The Python collections module provides specialized data structures like Counter and defaultdict, often used for counting word frequencies or managing token-to-index mappings. Additionally, pandas is imported for data manipulation, enabling the loading, cleaning, and transformation of tabular data, such as CSV files, into formats suitable for machine learning workflows. Together, these libraries streamline the process of preparing text data, creating datasets, and developing models for tasks like text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T20:15:27.280166Z",
     "iopub.status.busy": "2024-12-08T20:15:27.279729Z",
     "iopub.status.idle": "2024-12-08T20:15:27.286569Z",
     "shell.execute_reply": "2024-12-08T20:15:27.285279Z",
     "shell.execute_reply.started": "2024-12-08T20:15:27.280093Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import collections\n",
    "import torchtext\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T20:15:31.650819Z",
     "iopub.status.busy": "2024-12-08T20:15:31.650415Z",
     "iopub.status.idle": "2024-12-08T20:15:32.636551Z",
     "shell.execute_reply": "2024-12-08T20:15:32.635375Z",
     "shell.execute_reply.started": "2024-12-08T20:15:31.650786Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "classes = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration\n",
    "Let's explore the data so know the type of data in the dataset. We will also check if we have null values. From the dataset, the Title and Description have the information about the news article while Class Index has the class of the news article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T20:15:38.769879Z",
     "iopub.status.busy": "2024-12-08T20:15:38.769524Z",
     "iopub.status.idle": "2024-12-08T20:15:40.102960Z",
     "shell.execute_reply": "2024-12-08T20:15:40.101635Z",
     "shell.execute_reply.started": "2024-12-08T20:15:38.769848Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Visualize the train_df DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print('------------Description of the data-------------')\n",
    "print(train_df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print('\\n------------Missing values in the data-------------')\n",
    "print(train_df.isnull().sum())\n",
    "\n",
    "# Distribution of classes\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='Class Index', data=train_df)\n",
    "plt.title('Distribution of Classes in Training Data')\n",
    "plt.xlabel('Class Index')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has title and description for each news article, we will combine the Title and Description columns to have more vocabulary to help provide more context to the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T20:16:00.018307Z",
     "iopub.status.busy": "2024-12-08T20:16:00.017729Z",
     "iopub.status.idle": "2024-12-08T20:16:01.179613Z",
     "shell.execute_reply": "2024-12-08T20:16:01.178375Z",
     "shell.execute_reply.started": "2024-12-08T20:16:00.018270Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def combine_text(row):\n",
    "    return f\"{row['Title']} - {row['Description']}\"\n",
    "\n",
    "train_df['Text'] = train_df.apply(combine_text, axis=1)\n",
    "test_df['Text'] = test_df.apply(combine_text, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a custom dataset\n",
    "\n",
    "We will create a custom Pytorch dataset called NewsDataset for handling news articles stored in a DataFrame as a PyTorch Dataset object. It initializes with the DataFrame (df) and tracks the number of samples. The __getitem__ method retrieves the label and the combination of Title and Description by its index, returning a tuple containing the class label (Class Index) and the text (Text). The __len__ method provides the total number of samples in the dataset. This class enables easy integration with PyTorch’s DataLoader for batching and shuffling data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T20:16:07.295329Z",
     "iopub.status.busy": "2024-12-08T20:16:07.294184Z",
     "iopub.status.idle": "2024-12-08T20:16:07.301378Z",
     "shell.execute_reply": "2024-12-08T20:16:07.300066Z",
     "shell.execute_reply.started": "2024-12-08T20:16:07.295283Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "  def __init__(self,df):\n",
    "    self.n_samples = len(df)\n",
    "    self.dataframe = df\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    row = self.dataframe.iloc[index]\n",
    "    return row['Class Index'], row['Text']\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T20:16:09.658056Z",
     "iopub.status.busy": "2024-12-08T20:16:09.657315Z",
     "iopub.status.idle": "2024-12-08T20:16:09.663187Z",
     "shell.execute_reply": "2024-12-08T20:16:09.661805Z",
     "shell.execute_reply.started": "2024-12-08T20:16:09.658015Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# now we convert the dataframe for the training and testing into datasets\n",
    "train_dataset = NewsDataset(train_df)\n",
    "test_dataset = NewsDataset(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "Neural networks work with numbers and our dataset is made up of text, we will need to convert these texts into numbers to be able to pass them to a neural network and this process is called Vectorization. But before we do this, we will have to tokenize the dataset by breaking down text into smaller units, called tokens. These tokens can be words, subwords, or characters, depending on the tokenization method used.\n",
    "\n",
    "To convert text to numbers, we will need to build a vocabulary of all tokens. We first build the dictionary using the Counter object, and then create a Vocab object that would help us deal with vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T20:16:13.171288Z",
     "iopub.status.busy": "2024-12-08T20:16:13.170847Z",
     "iopub.status.idle": "2024-12-08T20:16:25.722548Z",
     "shell.execute_reply": "2024-12-08T20:16:25.721245Z",
     "shell.execute_reply.started": "2024-12-08T20:16:13.171251Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "counter = collections.Counter()\n",
    "for (label, line) in train_dataset:\n",
    "    counter.update(torchtext.data.utils.ngrams_iterator(tokenizer(line),ngrams=1))\n",
    "vocab = torchtext.vocab.Vocab(counter, min_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use the `vocab.stoi` dictionary from `torchtext` to convert strings into their numeric representations, with `stoi` standing for \"string to integer\". The is the `encode` function. Conversely, the vocab.itos dictionary allows us to convert numeric representations back into text by performing a reverse lookup as seen in the `decode` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T20:16:33.515746Z",
     "iopub.status.busy": "2024-12-08T20:16:33.515340Z",
     "iopub.status.idle": "2024-12-08T20:16:33.523315Z",
     "shell.execute_reply": "2024-12-08T20:16:33.522077Z",
     "shell.execute_reply.started": "2024-12-08T20:16:33.515712Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "print(f\"Vocab size if {vocab_size}\")\n",
    "\n",
    "def encode(x):\n",
    "    return [vocab.stoi[s] for s in tokenizer(x)]\n",
    "\n",
    "def decode(x):\n",
    "    return [vocab.itos[i] for i in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The padify function processes a batch of data for a PyTorch model. It takes a batch b (a list of tuples, where each tuple contains a label and a text sequence). The text sequences are encoded into numerical representations, and all sequences are padded to the same length using the maximum sequence length in the batch. The function returns a tuple containing two tensors: the first tensor holds the labels (adjusted by subtracting 1), and the second tensor contains the padded features. This ensures consistent input dimensions for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T20:16:36.859228Z",
     "iopub.status.busy": "2024-12-08T20:16:36.858818Z",
     "iopub.status.idle": "2024-12-08T20:16:36.866092Z",
     "shell.execute_reply": "2024-12-08T20:16:36.864837Z",
     "shell.execute_reply.started": "2024-12-08T20:16:36.859196Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def padify(b):\n",
    "    v = [encode(x[1]) for x in b]\n",
    "    l = max(map(len,v))\n",
    "    return ( # tuple of two tensors - labels and features\n",
    "        torch.LongTensor([t[0]-1 for t in b]),\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the RNN Classifier\n",
    "We will now define a simple RNN classifer. The RNNClassifier class is a PyTorch model for text classification. It consists of an embedding layer to convert tokens into dense vectors, a simple Recurrent Neural Network (RNN) to process sequential data, and a fully connected layer for mapping the RNN’s output to class probabilities. During the forward pass, the model computes embeddings, processes them through the RNN, applies mean pooling over the sequence, and passes the result through the classifier to predict the class. This model is designed for tasks involving sequential text inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T20:16:41.623406Z",
     "iopub.status.busy": "2024-12-08T20:16:41.623010Z",
     "iopub.status.idle": "2024-12-08T20:16:41.631825Z",
     "shell.execute_reply": "2024-12-08T20:16:41.630382Z",
     "shell.execute_reply.started": "2024-12-08T20:16:41.623374Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class RNNClassifier(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = torch.nn.RNN(embed_dim,hidden_dim,batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = self.embedding(x)\n",
    "        x,h = self.rnn(x)\n",
    "        return self.fc(x.mean(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T20:17:35.407375Z",
     "iopub.status.busy": "2024-12-08T20:17:35.406944Z",
     "iopub.status.idle": "2024-12-08T20:17:35.415382Z",
     "shell.execute_reply": "2024-12-08T20:17:35.414084Z",
     "shell.execute_reply.started": "2024-12-08T20:17:35.407339Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=padify, shuffle=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T20:18:08.295278Z",
     "iopub.status.busy": "2024-12-08T20:18:08.294752Z",
     "iopub.status.idle": "2024-12-08T20:28:57.993255Z",
     "shell.execute_reply": "2024-12-08T20:28:57.992249Z",
     "shell.execute_reply.started": "2024-12-08T20:18:08.295226Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = RNNClassifier(vocab_size,64,32,len(classes)).to(device)\n",
    "lr = 0.01\n",
    "report_freq=200\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "loss_fn = loss_fn.to(device)\n",
    "model.train()\n",
    "total_loss,acc,count,i = 0,0,0,0\n",
    "for labels,features in train_loader:\n",
    "    optimizer.zero_grad()\n",
    "    features, labels = features.to(device), labels.to(device)\n",
    "    out = model(features)\n",
    "    loss = loss_fn(out,labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    total_loss+=loss\n",
    "    _,predicted = torch.max(out,1)\n",
    "    acc+=(predicted==labels).sum()\n",
    "    count+=len(labels)\n",
    "    i+=1\n",
    "    if i%report_freq==0:\n",
    "        print(f\"{count}: acc={acc.item()/count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T20:29:23.093031Z",
     "iopub.status.busy": "2024-12-08T20:29:23.092639Z",
     "iopub.status.idle": "2024-12-08T20:29:23.109974Z",
     "shell.execute_reply": "2024-12-08T20:29:23.108690Z",
     "shell.execute_reply.started": "2024-12-08T20:29:23.092997Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, collate_fn=padify, shuffle=True)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (target, data) in enumerate(test_loader):\n",
    "\n",
    "        word_lookup = [vocab.itos[w] for w in data[batch_idx]]\n",
    "        unknow_vals = {'<unk>'}\n",
    "        word_lookup = [ele for ele in word_lookup if ele not in unknow_vals]\n",
    "        print('Input text:\\n {}\\n'.format(word_lookup))\n",
    "\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        pred = model(data)\n",
    "        print(torch.argmax(pred[batch_idx]))\n",
    "        print(\"Actual:\\nvalue={}, class_name= {}\\n\".format(target[batch_idx], classes[target[batch_idx]]))\n",
    "        print(\"Predicted:\\nvalue={}, class_name= {}\\n\".format(pred[0].argmax(0),classes[pred[0].argmax(0)]))\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 612351,
     "sourceId": 1095715,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
